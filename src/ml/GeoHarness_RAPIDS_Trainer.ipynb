{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GeoHarness v4.0: RAPIDS cuML / CPU Fallback Offset Decoder Training\n",
                "\n",
                "ì´ ë…¸íŠ¸ë¶ì€ `GeoHarness` í”„ë¡œì íŠ¸ì—ì„œ ìˆ˜ì§‘í•œ `ml_dataset.csv` íŒŒì¼ê³¼ `vworld_anchors.csv` íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ Google WGS84 ì¢Œí‘œ â†” Naver KATECH/TM128 ì¢Œí‘œ ê°„ì˜ **ë¹„ì„ í˜• ê³µê°„ ì™œê³¡ ì˜¤í”„ì…‹(Offset)** ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
                "\n",
                "### ğŸ‘‰ íŒ€ì› ê°€ì´ë“œ (ì¤‘ìš”)\n",
                "1. **Colab GPU / ë¡œì»¬ CPU ìë™ ì „í™˜**: ì´ ì½”ë“œëŠ” GPU(RAPIDS cuML)ê°€ ê°ì§€ë˜ë©´ ì´ˆê³ ì†ìœ¼ë¡œ í•™ìŠµí•˜ê³ , ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ Scikit-Learn(CPU)ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤. í˜„ì¬ 5,000ê±´ ë¯¸ë§Œ ë°ì´í„°ì´ë¯€ë¡œ ë¡œì»¬ CPUë¡œ ëŒë ¤ë„ ìˆ˜ ì´ˆ ë‚´ì— ëë‚©ë‹ˆë‹¤.\n",
                "2. **í•„ìˆ˜ ì…ë ¥ íŒŒì¼**: í”„ë¡œì íŠ¸ì˜ `data/` í´ë” ì•ˆì— `ml_dataset.csv`ì™€ `vworld_anchors.csv`ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
                "3. **ì¶œë ¥ íŒŒì¼**: ì‹¤í–‰ì´ ëë‚˜ë©´ `src/models/decoder.pkl` íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤. ì´ íŒŒì¼ì´ GeoHarness API ì„œë²„ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
                "!pip install scikit-learn pandas pyproj"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import math\n",
                "import csv\n",
                "import logging\n",
                "from pathlib import Path\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(\"RAPIDSTrainer\")\n",
                "\n",
                "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°\n",
                "current_dir = Path.cwd()\n",
                "while current_dir.name != 'GeoHarness' and current_dir.parent != current_dir:\n",
                "    current_dir = current_dir.parent\n",
                "\n",
                "if current_dir.name != 'GeoHarness':\n",
                "    current_dir = Path.cwd()  # Colab root\n",
                "\n",
                "print(f\"Project root determined as: {current_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GPU (cuML) / CPU (sklearn) ìë™ ê°ì§€\n",
                "try:\n",
                "    import cudf\n",
                "    import cuml\n",
                "    from cuml.ensemble import RandomForestRegressor as RF\n",
                "    from cuml.metrics import mean_squared_error\n",
                "    GPU_AVAILABLE = True\n",
                "    print(\"âœ… NVIDIA RAPIDS (cuML/cuDF) detected â€” GPU mode\")\n",
                "except ImportError:\n",
                "    import pandas as cudf\n",
                "    from sklearn.ensemble import RandomForestRegressor as RF\n",
                "    from sklearn.metrics import mean_squared_error\n",
                "    GPU_AVAILABLE = False\n",
                "    print(\"âš ï¸ RAPIDS not available â€” falling back to sklearn (CPU)\")\n",
                "\n",
                "try:\n",
                "    import joblib\n",
                "except ImportError:\n",
                "    import pickle as joblib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ê±°ë¦¬ ë° ë°©ìœ„ê° ê³„ì‚°)\n",
                "def haversine_distance(lat1, lng1, lat2, lng2):\n",
                "    R = 6371000\n",
                "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
                "    dphi = math.radians(lat2 - lat1)\n",
                "    dlam = math.radians(lng2 - lng1)\n",
                "    a = math.sin(dphi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlam / 2) ** 2\n",
                "    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
                "\n",
                "def bearing(lat1, lng1, lat2, lng2):\n",
                "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
                "    dlam = math.radians(lng2 - lng1)\n",
                "    x = math.sin(dlam) * math.cos(phi2)\n",
                "    y = math.cos(phi1) * math.sin(phi2) - math.sin(phi1) * math.cos(phi2) * math.cos(dlam)\n",
                "    return (math.degrees(math.atan2(x, y)) + 360) % 360\n",
                "\n",
                "def compute_anchor_features(g_lat, g_lng, anchors):\n",
                "    min_dist = float(\"inf\")\n",
                "    min_bearing = 0\n",
                "    for i, a in enumerate(anchors):\n",
                "        d = haversine_distance(g_lat, g_lng, a[\"lat\"], a[\"lng\"])\n",
                "        if d < min_dist:\n",
                "            min_dist = d\n",
                "            min_bearing = bearing(g_lat, g_lng, a[\"lat\"], a[\"lng\"])\n",
                "    return min_dist, min_bearing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. ë°ì´í„° ë° VWorld ì•ˆì»¤ ë¡œë“œ\n",
                "dataset_path = current_dir / \"data\" / \"ml_dataset.csv\"\n",
                "anchors_path = current_dir / \"data\" / \"vworld_anchors.csv\"\n",
                "\n",
                "if not dataset_path.exists():\n",
                "    raise FileNotFoundError(f\"âŒ ì˜¤ë¥˜: {dataset_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
                "\n",
                "df = cudf.read_csv(dataset_path)\n",
                "print(f\"Dataset loaded: {len(df)} rows\")\n",
                "\n",
                "anchors = []\n",
                "if anchors_path.exists():\n",
                "    with open(anchors_path, 'r', encoding='utf-8') as f:\n",
                "        reader = csv.DictReader(f)\n",
                "        for row in reader:\n",
                "            anchors.append({\n",
                "                \"name\": row[\"anchor_name\"],\n",
                "                \"lat\": float(row[\"vw_lat\"]),\n",
                "                \"lng\": float(row[\"vw_lng\"]),\n",
                "            })\n",
                "    print(f\"Loaded {len(anchors)} VWorld anchors\")\n",
                "else:\n",
                "    print(\"âš ï¸ vworld_anchors.csv not found, proceeding without anchor features.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. íƒ€ê²Ÿ ë° í”¼ì³ ìƒì„± (ë¸íƒ€ ë³€í™˜)\n",
                "# (ë§¤ìš° ì¤‘ìš”) ì›ë³¸ ì¢Œí‘œê³„ ìì²´ê°€ ì•„ë‹ˆë¼, 'êµ¬ê¸€ ì¢Œí‘œê°€ ë„¤ì´ë²„ ì¢Œí‘œì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ê°€(delta)'ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
                "df[\"delta_x\"] = df[\"n_mapx\"].astype(float) - df[\"g_lng\"]\n",
                "df[\"delta_y\"] = df[\"n_mapy\"].astype(float) - df[\"g_lat\"]\n",
                "\n",
                "if anchors:\n",
                "    anchor_dists = []\n",
                "    anchor_bearings = []\n",
                "    df_pd = df.to_pandas() if GPU_AVAILABLE else df\n",
                "    \n",
                "    for _, row in df_pd.iterrows():\n",
                "        dist, bear = compute_anchor_features(float(row[\"g_lat\"]), float(row[\"g_lng\"]), anchors)\n",
                "        anchor_dists.append(dist)\n",
                "        anchor_bearings.append(bear)\n",
                "        \n",
                "    if GPU_AVAILABLE:\n",
                "        import cudf as real_cudf\n",
                "        df[\"anchor_dist\"] = real_cudf.Series(anchor_dists)\n",
                "        df[\"anchor_bearing\"] = real_cudf.Series(anchor_bearings)\n",
                "    else:\n",
                "        df[\"anchor_dist\"] = anchor_dists\n",
                "        df[\"anchor_bearing\"] = anchor_bearings\n",
                "        \n",
                "    feature_cols = [\"g_lat\", \"g_lng\", \"anchor_dist\", \"anchor_bearing\"]\n",
                "else:\n",
                "    feature_cols = [\"g_lat\", \"g_lng\"]\n",
                "\n",
                "print(f\"ì„¤ì •ëœ Features: {feature_cols}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. ëª¨ë¸ í›ˆë ¨ (RandomForest)\n",
                "test_ratio = 0.1\n",
                "n_test = max(1, int(len(df) * test_ratio))\n",
                "n_train = len(df) - n_test\n",
                "\n",
                "X = df[feature_cols]\n",
                "y = df[[\"delta_x\", \"delta_y\"]]\n",
                "\n",
                "X_train, X_test = X.iloc[:n_train], X.iloc[n_train:]\n",
                "y_train, y_test = y.iloc[:n_train], y.iloc[n_train:]\n",
                "\n",
                "print(f\"Training split: {n_train} train / {n_test} test\")\n",
                "\n",
                "model_x = RF(n_estimators=200, random_state=42)\n",
                "model_y = RF(n_estimators=200, random_state=42)\n",
                "\n",
                "print(\"Training model_x (delta_x)...\")\n",
                "model_x.fit(X_train, y_train[\"delta_x\"])\n",
                "print(\"Training model_y (delta_y)...\")\n",
                "model_y.fit(X_train, y_train[\"delta_y\"])\n",
                "print(\"Training complete! âœ…\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. ì„±ëŠ¥ í‰ê°€\n",
                "pred_x = model_x.predict(X_test)\n",
                "pred_y = model_y.predict(X_test)\n",
                "\n",
                "if GPU_AVAILABLE:\n",
                "    mse_x = mean_squared_error(y_test[\"delta_x\"], pred_x).item()\n",
                "    mse_y = mean_squared_error(y_test[\"delta_y\"], pred_y).item()\n",
                "else:\n",
                "    mse_x = mean_squared_error(y_test[\"delta_x\"], pred_x)\n",
                "    mse_y = mean_squared_error(y_test[\"delta_y\"], pred_y)\n",
                "\n",
                "rmse_x = mse_x ** 0.5\n",
                "rmse_y = mse_y ** 0.5\n",
                "\n",
                "print(f\"--- ì˜¤í”„ì…‹ ì˜ˆì¸¡ ì˜¤ì°¨ (RMSE) ---\")\n",
                "print(f\"Xì¶• ì—ëŸ¬: {rmse_x:.6f}\")\n",
                "print(f\"Yì¶• ì—ëŸ¬: {rmse_y:.6f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. ëª¨ë¸ ë²ˆë“¤ ì €ì¥ (inference.py ê°€ ì½ì„ ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ Export)\n",
                "output_path = current_dir / \"src\" / \"models\" / \"decoder.pkl\"\n",
                "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "model_bundle = {\n",
                "    \"model_x\": model_x,\n",
                "    \"model_y\": model_y,\n",
                "    \"feature_cols\": feature_cols,\n",
                "    \"anchors\": anchors,\n",
                "    \"rmse_x\": rmse_x,\n",
                "    \"rmse_y\": rmse_y,\n",
                "    \"gpu_trained\": GPU_AVAILABLE,\n",
                "    \"n_samples\": len(df),\n",
                "}\n",
                "\n",
                "try:\n",
                "    joblib.dump(model_bundle, output_path)\n",
                "    print(f\"ğŸ‰ ì„±ê³µ: ëª¨ë¸ ë²ˆë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ -> {output_path}\")\n",
                "    print(\"ì´ ë…¸íŠ¸ë¶ì˜ ì‘ì—…ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ API ì„œë²„ì—ì„œ ML ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
                "except Exception as e:\n",
                "    print(f\"ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
                "    joblib.dump(model_bundle, 'decoder.pkl')\n",
                "    print(\"ë¡œì»¬ ë””ë ‰í† ë¦¬ì— decoder.pkl ë¡œ ëŒ€ì‹  ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
